{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome. As you probably know, deep learning has already transformed traditional internet businesses like web search and advertising. But deep learning is also enabling brand new products and businesses and ways of helping people to be created. Everything ranging from better healthcare, where deep learning is getting really good at reading X-ray images to delivering personalized education, to precision agriculture, to even self driving cars and many others. I\n",
    "\n",
    "AI is the new electricity **It transforms a lot of industries**\n",
    "\n",
    "So today, deep learning is one of the most highly sought after skills in technology worlds\n",
    "# Contents\n",
    "\n",
    "1. Neural Networks and Deep learnign\n",
    "\n",
    "2. Improving De\n",
    "\n",
    "3. the way you split your data into train, development or dev also called holdout cross-validation sets and test sets, has changed in the era of deep learning.\n",
    "4. Convolutional Networks \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is deep learning taking off?**\n",
    "\n",
    "\n",
    "Deep learning is taking off due to a large amount of data available through the digitization of the society,\n",
    "faster computation and innovation in the development of neural network algorithm.\n",
    "\n",
    "<img align='left' src='images/traditionalML.PNG' width='600'/>\n",
    "\n",
    "**Two things have to be considered to get to the high level of performance:**\n",
    "1. Being able to train a big enough neural network\n",
    "2. Huge amount of labeled data\n",
    "\n",
    "The process of training a neural network is iterative. It could take a good amount of time to train a neural network, which affects your productivity. Faster\n",
    "computation helps to iterate and improve new algorithm\n",
    "\n",
    "<img align='center' src='images/cycle.PNG' width='250'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning for Neural Network\n",
    "In supervised learning, we are given a data set and already know what our correct output should look like,\n",
    "having the idea that there is a relationship between the input and the output. \n",
    "\n",
    "Supervised learning problems are categorized into **\"regression\"** and **\"classification\"** problems. In a\n",
    "regression problem, we are trying to predict results within a continuous output, meaning that we are\n",
    "trying to map input variables to some continuous function. In a classification problem, we are instead\n",
    "trying to predict results in a discrete output. In other words, we are trying to map input variables into\n",
    "discrete categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align='left' src='images/sup_learning.PNG' width='550'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of neural network, for example **Convolution Neural Network (CNN)** used often\n",
    "for image application and **Recurrent Neural Network (RNN)** used for one-dimensional sequence data\n",
    "such as translating English to Chinses or a temporal component such as text transcript. As for the\n",
    "autonomous driving, it is a hybrid neural network architecture\n",
    "\n",
    "**Structured vs unstructured data**\n",
    "\n",
    "Structured data refers to things that has a defined meaning such as price, age whereas unstructured\n",
    "data refers to thing like pixel, raw audio, text.\n",
    "\n",
    "<img align='left' src='images/gato.PNG' width='700'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is neural network?\n",
    "\n",
    "It is a powerful learning algorithm inspired by how the brain works.\n",
    "\n",
    "**Example 1 ‚Äì single neural network** \n",
    "\n",
    "Given data about the size of houses on the real estate market and you want to fit a function that will\n",
    "predict their price. It is a linear regression problem because the price as a function of size is a continuous\n",
    "output.\n",
    "We know the prices can never be negative so we are creating a function called Rectified Linear Unit (ReLU)\n",
    "which starts at zero\n",
    "\n",
    "<img align='left' src='images/house.PNG' width='500'/>\n",
    "\n",
    "* The input is the size of the house (x)\n",
    "\n",
    "* The output is the price (y)\n",
    "\n",
    "* The ‚Äúneuron‚Äù implements the function ReLU (blue line)\n",
    "\n",
    "\n",
    "<img align='left' src='images/unared.PNG' width='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELU Activation function: \n",
    "\n",
    "* Density connected: As explained in this lecture, every input layer feature is interconnected with every hidden layer feature.\n",
    "\n",
    "<img align='left' src='images/ejemplo1.JPG' width='750'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifciation\n",
    "\n",
    "In a binary classification problem, the result is a discrete value output.\n",
    "\n",
    "For example - account hacked (1) or compromised (0)\n",
    "- a tumor malign (1) or benign (0)\n",
    "\n",
    "Example: Cat vs Non-Cat\n",
    "\n",
    "The goal is to train a classifier that the input is an image represented by a feature vector, ùë•, and predicts\n",
    "whether the corresponding label ùë¶ is 1 or 0. In this case, whether this is a cat image (1) or a non-cat image\n",
    "(0).\n",
    "\n",
    "<img align='center' src='images/binary.PNG' width='750'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is store in the computer in three separate matrices corresponding to the Red, Green, and Blue\n",
    "color channels of the image. The three matrices have the same size as the image, for example, the\n",
    "resolution of the cat image is 64 pixels X 64 pixels, the three matrices (RGB) are 64 X 64 each.\n",
    "\n",
    "The value in a cell represents the pixel intensity which will be used to create a feature vector of ndimension. In pattern recognition and machine learning, a feature vector represents an object, in this\n",
    "case, a cat or no cat.\n",
    "\n",
    "To create a feature vector, ùë•, the pixel intensity values will be ‚Äúunroll‚Äù or ‚Äúreshape‚Äù for each color. The\n",
    "dimension of the input feature vector ùë• is ùëõùë• = 64 ùë• 64 ùë• 3 = 12 288.\n",
    "\n",
    "<img align='left' src='images/rgb.PNG' width='150'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a learning algorithm used in a supervised learning problem when the output ùë¶ are\n",
    "all either zero or one. The goal of logistic regression is to minimize the error between its predictions and\n",
    "training data.\n",
    "\n",
    "Example: Cat vs No - cat\n",
    "\n",
    "Given an image represented by a feature vector ùë•, the algorithm will evaluate the probability of a cat\n",
    "being in that image\n",
    "\n",
    "$\\text {Given } x, \\hat{y}=P(y=1 | x), \\text { where } 0 \\leq \\hat{y} \\leq 1$\n",
    "\n",
    "The parameters used in Logistic regression are:\n",
    "\n",
    "*  The input features vector: $x \\epsilon R^{nx}$ , where ùëõùë• is the number of features\n",
    "* The training label: $\\text{y } \\epsilon \\text{ 0,1}$\n",
    "* The weights $w$ $\\epsilon R^{nx}$ ,\n",
    "* The threshold: b ‚àà R\n",
    "* The Output $\\hat{y} = \\sigma (w^Tx + b)$\n",
    "* Sigmoid function: $s= \\sigma (w^Tx + b)=$ $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "<img align='left' src='images/sigmoid.png' width='400'/>\n",
    "\n",
    "\n",
    "$(w^Tx + b)$ is a linear function $(ax+b)$, but since we are looking for a probability constraint between\n",
    "[0,1], the sigmoid function is used. The function is bounded between [0,1] as shown in the graph above.\n",
    "\n",
    "Some observations from the graph:\n",
    "* If ùëß is a large positive number, then ùúé(ùëß) = 1\n",
    "* If ùëß is small or large negative number, then ùúé(ùëß) = 0\n",
    "* If ùëß = 0, then ùúé(ùëß) = 0.5\n",
    "* $0 \\leq {y} \\leq 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Given } \\hat{y} = \\sigma (w^Tx + b)$\n",
    "$\\text {, interpret } \\hat{y}=P(y=1 | x)$\n",
    "* if $y=1 \\longrightarrow$ $P(y | x)=\\hat{y}$\n",
    "* if $y=0 \\longrightarrow$ $P(y | x)=1-\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "To train the parameters ùë§ and ùëè, we need to define a cost function.\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\hat{y}^{(i)}=\\sigma\\left(w^{T} x^{(i)}+b\\right), \\text { where } \\sigma\\left(z^{(i)}\\right)=\\frac{1}{1+e^{-z^{(i)}}}\\\\\n",
    "&\\text {Given }\\left\\{\\left(x^{(1)}, y^{(1)}\\right), \\cdots,\\left(x^{(m)}, y^{(m)}\\right)\\right\\}, \\text { we want } \\hat{y}^{(i)} \\approx y^{(i)}\n",
    "\\end{aligned}\n",
    "where $x^{(i)}$ the i-th training example\n",
    "\n",
    "**Loss (error) function:**\n",
    "\n",
    "The loss function measures the discrepancy between the prediction ($\\hat{y}^{(i)}$) and the desired output ($y^{(i)}$).\n",
    "In other words, the loss function computes the error for a single training example\n",
    "\n",
    "$L(\\hat{y}, y) = \\frac{1}{2} (\\hat{y} - y)^2$ **(Non - convex: Multiple local minimas)**\n",
    "\n",
    "$L\\left(\\hat{y}^{(i)}, y^{(i)}\\right)=-\\left(y^{(i)} \\log \\left(\\hat{y}^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{y}^{(i)}\\right)\\right.$\n",
    "* If $y^{(i)}=1: L\\left(\\hat{y}^{(i)}, y^{(i)}\\right)=-\\log \\left(\\hat{y}^{(i)}\\right)$ where $\\log \\left(\\hat{y}^{(i)}\\right)$ and $\\hat{y}^{(i)}$ should be close to 1\n",
    "* If $y^{(i)}=0: L\\left(\\hat{y}^{(i)}, y^{(i)}\\right)=-\\log \\left(1-\\hat{y}^{(i)}\\right)$ where $\\log \\left(1-\\hat{y}^{(i)}\\right)$ and $\\hat{y}^{(i)}$ should be close to 0\n",
    "\n",
    "**Cost function**\n",
    "\n",
    "The cost function is the average of the loss function of the entire ```m``` training set. We are going to find the parameters ```ùë§``` ùëéùëõùëë ```ùëè``` that minimize the overall cost function.\n",
    "\n",
    "\\begin{aligned} J(w, b)=\\frac{1}{m} \\sum_{i=1}^{m} L\\left(\\hat{y}^{(i)}, y^{(i)}\\right)=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[\\left(y^{(i)} \\log \\left(\\hat{y}^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{y}^{(i)}\\right)\\right]\\right.\\end{aligned}\n",
    "\n",
    "### Gradient Descendent \n",
    "\n",
    "Want to find ùë§,ùëè that minimize ùêΩ(ùë§,ùëè)  \n",
    "\n",
    "\n",
    "<img align='left' src='images/global.png' width='400'/>\n",
    "\n",
    "$w : = w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}$\n",
    "\n",
    "$w : = w - \\alpha \\frac{\\partial J(w, b)}{\\partial b}$\n",
    "\n",
    "\n",
    "$dw = \\frac{\\partial J(w, b)}{\\partial w}$\n",
    "\n",
    "$db = \\frac{\\partial J(w, b)}{\\partial b}$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $:=$ is an update assignation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs \n",
    "\n",
    "\n",
    "\n",
    "Computational graphs are a nice way to think about mathematical expressions. For example, consider the expression:\n",
    "$$e=(a+b)‚àó(b+1)$$ There are three operations: two additions and one multiplication. To help us talk about this, let‚Äôs introduce two intermediary variables, ```c``` and ```d``` so that every function‚Äôs output has a variable. We now have:\n",
    "\n",
    "$$c=a+b$$\n",
    "\n",
    "$$d=b+1$$\n",
    "\n",
    "$$e=c‚àód$$\n",
    "\n",
    "To create a computational graph, we make each of these operations AND the input variables into nodes. When one node‚Äôs value is the input to another node, an arrow goes from one to another. We can evaluate the expression by setting the input variables to certain values and computing nodes up through the graph. For example, let‚Äôs set $a=2$ and $b=1$:\n",
    "\n",
    "<img align='left' src='images/computacionalgraf.PNG' width='550'/>\n",
    "\n",
    "To evaluate the partial derivatives in this graph, we need the sum rule and the product rule:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\frac{\\partial}{\\partial a}(a+b)=\\frac{\\partial a}{\\partial a}+\\frac{\\partial b}{\\partial a}=1\\\\\\\\\n",
    "&\\frac{\\partial}{\\partial u} u v=u \\frac{\\partial v}{\\partial u}+v \\frac{\\partial u}{\\partial u}=v\n",
    "\\end{aligned}$$\n",
    "\n",
    "If a directly affects c, then we want to know how it affects c. If a changes a little bit, how does c change? We call this the partial derivative of c with respect to a. **we want to understand how nodes that aren‚Äôt directly connected affect each other**\n",
    "\n",
    "The general rule is to sum over all possible paths from one node to the other, multiplying the derivatives on each edge of the path together.\n",
    "‚Äúsum over paths‚Äù rule is just a different way of thinking about the **multivariate chain rule**. For example:\n",
    "\n",
    "$$\\frac{\\partial e}{\\partial b}=\\frac{\\partial e}{\\partial c}\\frac{\\partial c}{\\partial b}+\\frac{\\partial e}{\\partial d}\\frac{\\partial d}{\\partial b}=1*2+1*3$$\n",
    "\n",
    "---\n",
    "#### Factoring Paths\n",
    "\n",
    "The problem with just ‚Äúsumming over the paths‚Äù is that it‚Äôs very easy to get a combinatorial explosion in the number of possible paths.\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial X}=\\alpha \\delta+\\alpha \\epsilon+\\alpha \\zeta+\\beta \\delta+\\beta \\epsilon+\\beta \\zeta+\\gamma \\delta+\\gamma \\epsilon+\\gamma \\zeta$$\n",
    "\n",
    "The above only has nine paths, but it would be easy to have the number of paths to grow exponentially as the graph becomes more complicated.\n",
    "Instead of just naively summing over the paths, it would be much better to factor them:\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial X}=(\\alpha+\\beta+\\gamma)(\\delta+\\epsilon+\\zeta)$$\n",
    "\n",
    "This is where **‚Äúforward-mode differentiation‚Äù and ‚Äúreverse-mode differentiation‚Äù** come in. They‚Äôre algorithms for efficiently computing the sum by factoring the paths. Instead of summing over all of the paths explicitly, they compute the same sum more efficiently by merging paths back together at every node. In fact, **both algorithms touch each edge exactly once**\n",
    "\n",
    "1. forward-mode differentiation: it starts at an input to the graph and moves towards the end. At every node, it sums all the paths feeding in. Each of those paths represents one way in which the input affects that node. By adding them up, we get the total way in which the node is affected by the input, it‚Äôs derivative. Its is similar to calculus class.\n",
    "Applies $\\frac{\\partial}{\\partial X}$to every node.**This gives us the derivative of every node with respect to input.** giving us the derivative of our output with respect to a single input\n",
    "2. Reverse-mode differentiation, on the other hand, starts at an output of the graph and moves towards the beginning. At each node, it merges all paths which originated at that node. Applies $\\frac{\\partial Z}{\\partial}$ to every node. Similar to dynamic programming. **This gives us the derivative of Z or output with respect to every node.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img align='center' src='images/backpropag.png' width='500'/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this class, the coding convention ```dvar``` represent the derivative of a final output variable  such as ```J``` with respect to various intermediate quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized version:2.9935836791992188 ms\n",
      "For Loop:576.96533203125 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "tic = time.time() \n",
    "c=np.dot(a, b) \n",
    "toc  =  time.time()\n",
    "print(\"Vectorized version:\" + str(1000*(toc-tic)) +\" ms\")\n",
    "\n",
    "c=0 \n",
    "tic  =  time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc  =  time.time()\n",
    "print(\"For Loop:\" + str(1000*(toc-tic)) +\" ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
