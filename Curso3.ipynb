{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Strategy \n",
    "* Collect more data\n",
    "* Collect more diverse trainign set\n",
    "* Train algorithm longer with gradient descetn\n",
    "* Try adam isntead of gradient descent\n",
    "* Try bigger networks\n",
    "* Try smaller networks\n",
    "* Try dropout\n",
    "* Add L2 regularizati√≥n\n",
    "* Network architecture\n",
    "* Network archicteture \n",
    "    - Activvation\n",
    "    - \\# hidden units\n",
    "    \n",
    "# Orthogonalization  \n",
    "For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that four things hold true. \n",
    "1. **Fit training set well on cost function** First, is that you usually have to make sure that you're at least doing well on the training set. So performance on the training set needs to pass some acceptability assessment. For some applications, this might mean doing comparably to human level performance. But this will depend on your application, and we'll talk more about comparing to human level performance next week.\n",
    "2. **Fit dev set well on cost function**\n",
    "3. **Fit test set well on cost function**\n",
    "3. **Performs well in real world**\n",
    "\n",
    "el priemro se soluccion con bigget network, the optiization algorithm\n",
    "el segundo con regularization o con un bigger traingin set\n",
    "et tres con bigger dev set\n",
    "y el cuarto cambiando el dev set o la cost function\n",
    "\n",
    " The exact details of what's precision and recall don't matter too much for this example. But briefly, the definition of precision is, of the examples that your classifier recognizes as cats,\n",
    "Play video starting at 1 minute 23 seconds and follow transcript1:23\n",
    "What percentage actually are cats?\n",
    "Play video starting at 1 minute 32 seconds and follow transcript1:32\n",
    "So if classifier A has 95% precision, this means that when classifier A says something is a cat, there's a 95% chance it really is a cat. And recall is, of all the images that really are cats, what percentage were correctly recognized by your classifier? So what percentage of actual cats, Are correctly recognized?\n",
    "\n",
    "<img align='center' src='images/metric.PNG' width='650'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  I often recommend that you set up a single real number evaluation metric for your problem. Let's look at an example.\n",
    "\n",
    "\n",
    "![image.png](attachment:fd2ccc80-2390-464d-8c37-9bcdb53569b5.png)\n",
    "\n",
    "\n",
    "precision: the examples that your classifier recognizes as cats, What percentage actually are cats\n",
    "o if classifier A has 95% precision, this means that when classifier A says something is a cat, there's a 95% chance it really is a cat.\n",
    "\n",
    "recall: of all the images that really are cats, what percentage were correctly recognized by your classifier? So what percentage of actual cats, Are correctly recognized? So if classifier A is 90% recall, this means that of all of the images in, say, your dev sets that really are cats, classifier A accurately pulled out 90% of them. \n",
    "\n",
    "\n",
    "trade-off between precision and recall\n",
    "\n",
    "\n",
    "The problem with using precision recall as your evaluation metric is that if classifier A does better on recall, which it does here, the classifier B does better on precision, then you're not sure which classifier is better.\n",
    "\n",
    "you just have to find a new evaluation metric that combines precision and recall.\n",
    " \n",
    "In the machine learning literature, the standard way to combine precision and recall is something called an F1 score. Think as average of precision (P) and recall\n",
    "\n",
    "\n",
    "$$F1 = \\frac{2}{\\frac{1}{P}+\\frac{1}{R}}$$ Harmonic mean of precition P and Recall R\n",
    "\n",
    "what I recommend in this example is, in addition to tracking your performance in the four different geographies, to also compute the average. And assuming that average performance is a reasonable single real number evaluation metric, by computing the average, you can quickly tell that it looks like algorithm C has a lowest average error.\n",
    "\n",
    "---\n",
    "\n",
    "**Satisficing and Optimizing metric**\n",
    "\n",
    "To summarize, if there are multiple things you care about by say there's one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you'll be satisfice. Almost it does better than some threshold you can now have an almost automatic way of quickly looking at multiple core size and picking the, quote, best one. Now these evaluation matrix must be evaluated or calculated on a training set or a development set or maybe on the test set. So one of the things you also need to do is set up training, dev or development, as well as test sets. In the next video, I want to share with you some guidelines for how to set up training, dev, and test sets. So let's go on to the next.\n",
    "\n",
    "cost = accuracy - 0.5 * running time\n",
    "\n",
    "maximize accuracy but subject \n",
    "\n",
    "that maximizes accuracy but subject to that the running time, that is the time it takes to classify an image, that that has to be less than or equal to 100 milliseconds. \n",
    "\n",
    "that running time is what we call a satisficing metric\n",
    "\n",
    "So in this case accuracy is the optimizing metric and a number of false positives every 24 hours is the satisficing metric\n",
    "\n",
    "**Train/dev/test distributions**\n",
    "\n",
    "The way you set up your training dev, or development sets and test sets, can have a huge impact on how rapidly you or your team can make progress on building machine learning application.\n",
    "\n",
    "* Dev set So, that dev set is also called the development set, or sometimes called the hold out cross validation set. And, workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep iterating to improve dev set performance until, finally, you have one clause that you're happy with that you then evaluate on your test set.\n",
    "* choose a dev set and test set to reflect data you expect to get in future and consider important to do well on. And, in particular, the dev set and the test set here, should come from the same distribution. So, whatever type of data you expect to get in the future, and once you do well on, try to get data that looks like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of Dev set\n",
    "\n",
    "* So if you had a hundred examples in total, these 70/30 or 60/20/20 rule of thumb would be pretty reasonable. If you had thousand examples, maybe if you had ten thousand examples, these heuristics are not unreasonable.\n",
    "\n",
    "* say you have a million training examples. it might be quite reasonable to set up your data so that you have 98% in the training set, 1% dev, and 1% test.\n",
    "\n",
    "\n",
    "## Size of test set\n",
    "\n",
    "* Set your test set to be enough to give high confidence in the overall performance of your system.\n",
    "\n",
    "* Maybe all you need is a train and dev set, And I think, not having a test set might be okay\n",
    "\n",
    "* I do find it reassuring to have a separate test set you can use to get an unbiased estimate of how I was doing before you shift it, but if you have a very large dev set so that you think you won't overfit the dev set too bad\n",
    "\n",
    "So to summarize, in the era of big data, I think the old rule of thumb of a 70/30 is that, that no longer applies. And the trend has been to use more data for training and less for dev and test, especially when you have a very large data sets. And the rule of thumb is really to try to set the dev set to big enough for its purpose, which helps you evaluate different ideas and pick this up from AOP better. And the purpose of test set is to help you evaluate your final cost buys. You just have to set your test set big enough for that purpose, and that could be much less than 30% of the data. So, I hope that gives some guidance or some suggestions on how to set up your dev and test sets in the Deep Learning era. Next, it turns out that sometimes, part way through a machine learning problem, you might want to change your evaluation metric, or change your dev and test sets. Let's talk about it when you might want to do that.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to change dev/test sets and metrics\n",
    "\n",
    "You've seen how set to have a dev set and evaluation metric is like placing a target somewhere for your team to aim at. \n",
    "\n",
    "**Orthogonalization for better performance**\n",
    "1. PLace target\n",
    "\n",
    "$$Error = \\frac{1}{\\sum_i w^{(i)}}\\sum_i w^{(i)}L\\{Y_{pred}^{(i)}, y^{(i)} \\}$$\n",
    "\n",
    "$w^{(i)}$ = 1 if $x^{(i)}$ is non-porn  \n",
    "$w^{(i)}$ = 10 if $x^{(i)}$ is porn\n",
    "\n",
    "*Orthogonalization for cat picture: anti-porn*\n",
    "* So far weve only discussed how to define a metric to evaluate classifiers (Place the target)\n",
    "* Worry separately about how to do wel on this metric\n",
    "\n",
    "---\n",
    "\n",
    "Bayes optimal error. Best posible error. That can't never being surpass.\n",
    "\n",
    "*Why compare to human-level performance*\n",
    "Humans are quite good a lot of task. So long as ML is worse than human, you can:\n",
    "* get labeled data from humans\n",
    "* Gain insight from manual error analysis. Why did a person get this right\n",
    "* Better analysis of bias/variances\n",
    "\n",
    "# Avoidable Bias\n",
    "\n",
    "* o the fact that there's a huge gap between how well your algorithm does on your training set versus how humans do shows that your algorithm isn't even fitting the training set well. So in terms of tools to reduce bias or variance, in this case I would say focus on reducing bias. So you want to do things like train a bigger neural network or run training set longer, just try to do better on the training set\n",
    "\n",
    "* Avoidable bias: difference between bayes and training error. You don't actually want to get below Bayes error.\n",
    "\n",
    "* Variance: Difference betweem training error and dev error.\n",
    "\n",
    "* Focus on either bias or variance reduction techniques\n",
    "\n",
    "<img align='center' src='images/AvoidableBIAS.PNG' width='650'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Human Level Performance\n",
    "\n",
    "**Proxy for Bayes Error**\"\n",
    "\n",
    "So to recap, having an estimate of human-level performance gives you an estimate of Bayes error. And this allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm.\n",
    "Suppose:\n",
    "\n",
    "### Surpassing human-level performance\n",
    "\n",
    "* once you've surpassed this 0.5% threshold, your options, your ways of making progress on the machine learning problem are just less clear. It doesn't mean you can't make progress, you might still be able to make significant progress, but some of the tools you have for pointing you in a clear direction just don't work as well\n",
    "\n",
    "*Problems where ML significantly surpasses human -level performance*\n",
    "* Online advertising\n",
    "* Product recommendations\n",
    "* Logistic (predicting transit time)\n",
    "* Loan approvals\n",
    "\n",
    "1. All this examples are actually learning from structured data. Where you might have a databse of waht has users clicked on. This are not actual perception problems. These are not computer vision\n",
    "\n",
    "\n",
    "2. today there are speech recognition systems that can surpass human-level performance. And there are also some computer vision, some image recognition tasks, where computers have surpassed human-level performance\n",
    "\n",
    "3. Medical\n",
    "\n",
    "       ECGs, skin cancer, narrow radiology task\n",
    "       \n",
    "## Improving your model performance\n",
    "\n",
    "**supervised learning algorithm to work well**\n",
    "1. You can fit the training set pretty well \n",
    "    - LOW Avoidable bias\n",
    "    - Problem is solved by training a bigger network or training longer\n",
    "2. The Training set performance generalizes pretty well to the dev/test set\n",
    "    - Variance\n",
    "    - Problem is solved regularization or getting more training data that could help you generalize better to dev set dat\n",
    "    \n",
    "**Steps.**\n",
    "\n",
    "1. looking at the difference between your training error and your proxy for Bayes error and just gives you a sense of the avoidable bias. In other words, just how much better do you think you should be trying to do on your training set. \n",
    "2. And then look at the difference between your dev error and your training error as an estimate of how much of a variance problem you have. In other words, how much harder you should be working to make your performance generalized from the training set to the dev set that it wasn't trained on explicitly.\n",
    "\n",
    "<img align='left' src='images/ruleofthumb.PNG' width='650'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 9, 9, 9, 9, 9, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[9]*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
