{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Strategy \n",
    "* Collect more data\n",
    "* Collect more diverse trainign set\n",
    "* Train algorithm longer with gradient descetn\n",
    "* Try adam isntead of gradient descent\n",
    "* Try bigger networks\n",
    "* Try smaller networks\n",
    "* Try dropout\n",
    "* Add L2 regularizati√≥n\n",
    "* Network architecture\n",
    "* Network archicteture \n",
    "    - Activvation\n",
    "    - \\# hidden units\n",
    "    \n",
    "# Orthogonalization  \n",
    "For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that four things hold true. \n",
    "1. **Fit training set well on cost function** First, is that you usually have to make sure that you're at least doing well on the training set. So performance on the training set needs to pass some acceptability assessment. For some applications, this might mean doing comparably to human level performance. But this will depend on your application, and we'll talk more about comparing to human level performance next week.\n",
    "2. **Fit dev set well on cost function**\n",
    "3. **Fit test set well on cost function**\n",
    "3. **Performs well in real world**\n",
    "\n",
    "el priemro se soluccion con bigget network, the optiization algorithm\n",
    "el segundo con regularization o con un bigger traingin set\n",
    "et tres con bigger dev set\n",
    "y el cuarto cambiando el dev set o la cost function\n",
    "\n",
    " The exact details of what's precision and recall don't matter too much for this example. But briefly, the definition of precision is, of the examples that your classifier recognizes as cats,\n",
    "Play video starting at 1 minute 23 seconds and follow transcript1:23\n",
    "What percentage actually are cats?\n",
    "Play video starting at 1 minute 32 seconds and follow transcript1:32\n",
    "So if classifier A has 95% precision, this means that when classifier A says something is a cat, there's a 95% chance it really is a cat. And recall is, of all the images that really are cats, what percentage were correctly recognized by your classifier? So what percentage of actual cats, Are correctly recognized?\n",
    "\n",
    "<img align='center' src='images/metric.PNG' width='650'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  I often recommend that you set up a single real number evaluation metric for your problem. Let's look at an example.\n",
    "\n",
    "precision: the examples that your classifier recognizes as cats, What percentage actually are cats\n",
    "o if classifier A has 95% precision, this means that when classifier A says something is a cat, there's a 95% chance it really is a cat.\n",
    "\n",
    "recall: of all the images that really are cats, what percentage were correctly recognized by your classifier? So what percentage of actual cats, Are correctly recognized? So if classifier A is 90% recall, this means that of all of the images in, say, your dev sets that really are cats, classifier A accurately pulled out 90% of them. \n",
    "\n",
    "\n",
    "trade-off between precision and recall\n",
    "\n",
    "\n",
    "The problem with using precision recall as your evaluation metric is that if classifier A does better on recall, which it does here, the classifier B does better on precision, then you're not sure which classifier is better.\n",
    "\n",
    "you just have to find a new evaluation metric that combines precision and recall.\n",
    " \n",
    "In the machine learning literature, the standard way to combine precision and recall is something called an F1 score. Think as average of precision (P) and recall\n",
    "\n",
    "\n",
    "$$F1 = \\frac{2}{\\frac{1}{P}+\\frac{1}{R}}$$ Harmonic mean of precition P and Recall R\n",
    "\n",
    "what I recommend in this example is, in addition to tracking your performance in the four different geographies, to also compute the average. And assuming that average performance is a reasonable single real number evaluation metric, by computing the average, you can quickly tell that it looks like algorithm C has a lowest average error.\n",
    "\n",
    "---\n",
    "\n",
    "**Satisficing and Optimizing metric**\n",
    "\n",
    "To summarize, if there are multiple things you care about by say there's one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you'll be satisfice. Almost it does better than some threshold you can now have an almost automatic way of quickly looking at multiple core size and picking the, quote, best one. Now these evaluation matrix must be evaluated or calculated on a training set or a development set or maybe on the test set. So one of the things you also need to do is set up training, dev or development, as well as test sets. In the next video, I want to share with you some guidelines for how to set up training, dev, and test sets. So let's go on to the next.\n",
    "\n",
    "cost = accuracy - 0.5 * running time\n",
    "\n",
    "maximize accuracy but subject \n",
    "\n",
    "that maximizes accuracy but subject to that the running time, that is the time it takes to classify an image, that that has to be less than or equal to 100 milliseconds. \n",
    "\n",
    "that running time is what we call a satisficing metric\n",
    "\n",
    "So in this case accuracy is the optimizing metric and a number of false positives every 24 hours is the satisficing metric\n",
    "\n",
    "**Train/dev/test distributions**\n",
    "\n",
    "The way you set up your training dev, or development sets and test sets, can have a huge impact on how rapidly you or your team can make progress on building machine learning application.\n",
    "\n",
    "* Dev set So, that dev set is also called the development set, or sometimes called the hold out cross validation set. And, workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep iterating to improve dev set performance until, finally, you have one clause that you're happy with that you then evaluate on your test set.\n",
    "* choose a dev set and test set to reflect data you expect to get in future and consider important to do well on. And, in particular, the dev set and the test set here, should come from the same distribution. So, whatever type of data you expect to get in the future, and once you do well on, try to get data that looks like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of Dev set\n",
    "\n",
    "* So if you had a hundred examples in total, these 70/30 or 60/20/20 rule of thumb would be pretty reasonable. If you had thousand examples, maybe if you had ten thousand examples, these heuristics are not unreasonable.\n",
    "\n",
    "* say you have a million training examples. it might be quite reasonable to set up your data so that you have 98% in the training set, 1% dev, and 1% test.\n",
    "\n",
    "\n",
    "## Size of test set\n",
    "\n",
    "* Set your test set to be enough to give high confidence in the overall performance of your system.\n",
    "\n",
    "* Maybe all you need is a train and dev set, And I think, not having a test set might be okay\n",
    "\n",
    "* I do find it reassuring to have a separate test set you can use to get an unbiased estimate of how I was doing before you shift it, but if you have a very large dev set so that you think you won't overfit the dev set too bad\n",
    "\n",
    "So to summarize, in the era of big data, I think the old rule of thumb of a 70/30 is that, that no longer applies. And the trend has been to use more data for training and less for dev and test, especially when you have a very large data sets. And the rule of thumb is really to try to set the dev set to big enough for its purpose, which helps you evaluate different ideas and pick this up from AOP better. And the purpose of test set is to help you evaluate your final cost buys. You just have to set your test set big enough for that purpose, and that could be much less than 30% of the data. So, I hope that gives some guidance or some suggestions on how to set up your dev and test sets in the Deep Learning era. Next, it turns out that sometimes, part way through a machine learning problem, you might want to change your evaluation metric, or change your dev and test sets. Let's talk about it when you might want to do that.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to change dev/test sets and metrics\n",
    "\n",
    "You've seen how set to have a dev set and evaluation metric is like placing a target somewhere for your team to aim at. \n",
    "\n",
    "**Orthogonalization for better performance**\n",
    "1. PLace target\n",
    "\n",
    "$$Error = \\frac{1}{\\sum_i w^{(i)}}\\sum_i w^{(i)}L\\{Y_{pred}^{(i)}, y^{(i)} \\}$$\n",
    "\n",
    "$w^{(i)}$ = 1 if $x^{(i)}$ is non-porn  \n",
    "$w^{(i)}$ = 10 if $x^{(i)}$ is porn\n",
    "\n",
    "*Orthogonalization for cat picture: anti-porn*\n",
    "* So far weve only discussed how to define a metric to evaluate classifiers (Place the target)\n",
    "* Worry separately about how to do wel on this metric\n",
    "\n",
    "---\n",
    "\n",
    "Bayes optimal error. Best posible error. That can't never being surpass.\n",
    "\n",
    "*Why compare to human-level performance*\n",
    "Humans are quite good a lot of task. So long as ML is worse than human, you can:\n",
    "* get labeled data from humans\n",
    "* Gain insight from manual error analysis. Why did a person get this right\n",
    "* Better analysis of bias/variances\n",
    "\n",
    "# Avoidable Bias\n",
    "\n",
    "* o the fact that there's a huge gap between how well your algorithm does on your training set versus how humans do shows that your algorithm isn't even fitting the training set well. So in terms of tools to reduce bias or variance, in this case I would say focus on reducing bias. So you want to do things like train a bigger neural network or run training set longer, just try to do better on the training set\n",
    "\n",
    "* Avoidable bias: difference between bayes and training error. You don't actually want to get below Bayes error.\n",
    "\n",
    "* Variance: Difference betweem training error and dev error.\n",
    "\n",
    "* Focus on either bias or variance reduction techniques\n",
    "\n",
    "<img align='center' src='images/AvoidableBIAS.PNG' width='650'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Human Level Performance\n",
    "\n",
    "**Proxy for Bayes Error**\"\n",
    "\n",
    "So to recap, having an estimate of human-level performance gives you an estimate of Bayes error. And this allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm.\n",
    "Suppose:\n",
    "\n",
    "### Surpassing human-level performance\n",
    "\n",
    "* once you've surpassed this 0.5% threshold, your options, your ways of making progress on the machine learning problem are just less clear. It doesn't mean you can't make progress, you might still be able to make significant progress, but some of the tools you have for pointing you in a clear direction just don't work as well\n",
    "\n",
    "*Problems where ML significantly surpasses human -level performance*\n",
    "* Online advertising\n",
    "* Product recommendations\n",
    "* Logistic (predicting transit time)\n",
    "* Loan approvals\n",
    "\n",
    "1. All this examples are actually learning from structured data. Where you might have a databse of waht has users clicked on. This are not actual perception problems. These are not computer vision\n",
    "\n",
    "\n",
    "2. today there are speech recognition systems that can surpass human-level performance. And there are also some computer vision, some image recognition tasks, where computers have surpassed human-level performance\n",
    "\n",
    "3. Medical\n",
    "\n",
    "       ECGs, skin cancer, narrow radiology task\n",
    "       \n",
    "## Improving your model performance\n",
    "\n",
    "**supervised learning algorithm to work well**\n",
    "1. You can fit the training set pretty well \n",
    "    - LOW Avoidable bias\n",
    "    - Problem is solved by training a bigger network or training longer\n",
    "2. The Training set performance generalizes pretty well to the dev/test set\n",
    "    - Variance\n",
    "    - Problem is solved regularization or getting more training data that could help you generalize better to dev set dat\n",
    "    \n",
    "**Steps.**\n",
    "\n",
    "1. looking at the difference between your training error and your proxy for Bayes error and just gives you a sense of the avoidable bias. In other words, just how much better do you think you should be trying to do on your training set. \n",
    "2. And then look at the difference between your dev error and your training error as an estimate of how much of a variance problem you have. In other words, how much harder you should be working to make your performance generalized from the training set to the dev set that it wasn't trained on explicitly.\n",
    "\n",
    "<img align='left' src='images/ruleofthumb.PNG' width='650'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 9, 9, 9, 9, 9, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[9]*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrying out error analysis\n",
    "\n",
    "If you're trying to get a learning algorithm to do a task that humans can do. And if your learning algorithm is not yet at the performance of a human. Then manually examining mistakes that your algorithm is making, can give you insights into what to do next. This process is called error analysis. \n",
    "\n",
    "In machine learning, sometimes we call this the ceiling on performance. Which just means, what's in the best case? How well could working on the dog problem help you?\n",
    "\n",
    " error analysis, can save you a lot of time. In terms of deciding what's the most important, or what's the most promising direction to focus on.\n",
    " \n",
    "  In this slide, we'll describe using error analysis to evaluate whether or not a single idea, dogs in this case, is worth working on.\n",
    " \n",
    "*Look at dev examples to evaluate ideas*\n",
    "\n",
    "Error analysis:\n",
    "* Get ~100 mislabeled dev set examples\n",
    "* count up how many are dogs\n",
    "\n",
    "5/100\n",
    "\n",
    "\"Ceiling\" upper bound on how much you could improve performance \n",
    "\n",
    "In other case 50/100 are dogs, it is worth spending time on the dog problem.\n",
    "\n",
    "\n",
    "*Evaluate multiple idea in parallel*\n",
    "\n",
    "Ideas for cat detection:\n",
    "* Fix pictures of dogs being recognized as cats\n",
    "* Fix great cats (lions, panther, ) being misrecognized\n",
    "* Improve performance on blurry images\n",
    " \n",
    " \n",
    "#  Cleaning up incorrectly labeled data\n",
    "\n",
    "**deep learning algorithms are quite robust to random errors in the training set.**\n",
    "\n",
    "* They are less robust to systematic errors.\n",
    "\n",
    "So for example, if your labeler consistently labels white dogs as cats, then that is a problem because your classifier will learn to classify all white colored dogs as cats\n",
    "\n",
    "\n",
    "**here are a few additional guidelines or principles to consider**\n",
    "\n",
    "* f you're going in to fix something on the dev set, I would apply the same process to the test set to make sure that they continue to come from the same distribution\n",
    "\n",
    "* It's super important that your dev and test sets come from the same distribution.\n",
    "\n",
    "# Build your first system quickly, then iterate\n",
    "\n",
    "\n",
    "If you're working on a brand new machine learning application, one of the piece of advice I often give people is that, I think you should build your first system quickly and then iterate. your main goal is to build something that works, as opposed to if your main goal is to invent a new machine learning algorithm which is a different goal, then your main goal is to get something that works really well. I'd encourage you to build something quick and dirty. Use that to do bias/variance analysis, use that to do error analysis and use the results of those analysis to help you prioritize where to go next.\n",
    "\n",
    "* Set up dev/set and metric\n",
    "* Build initial system quickly\n",
    "* Use bias/variance analysis & error analysis to prioritize next steps\n",
    "\n",
    "# Training and testing on different distributions\n",
    "et. So in this video, you've seen a couple examples of when allowing your training set data to come from a different distribution than your dev and test set allows you to have much more training data. And in these examples, it will cause your learning algorithm to perform better. Now one question you might ask is, should you always use all the data you have? The answer is subtle, it is not always yes.\n",
    "\n",
    "\n",
    "# Bias and Variance with mismatched data distributions\n",
    "\n",
    "Previously we had set up some training sets and some dev sets and some test sets as follows. And the dev and test sets have the same distribution, but the training sets will have some different distribution. What we're going to do is randomly shuffle the training sets and then carve out just a piece of the training set to be the training-dev set. So just as the dev and test set have the same distribution, the training set and the training-dev set, also have the same distribution.\n",
    "\n",
    "**Key QUantities**\n",
    "- HUman Level error\n",
    "- Train set error\n",
    "- Train dev - set error\n",
    "- Dev error\n",
    "\n",
    "\n",
    "<img align='center' src='images/biasmismatch.PNG' width='400'/>\n",
    "\n",
    "\n",
    "**More general formulation**\n",
    "\n",
    "So what we've seen is that by using training data that can come from a different distribution as a dev and test set, this could give you a lot more data and therefore help the performance of your learning algorithm. But instead of just having bias and variance as two potential problems, you now have this third potential problem, data mismatch. So what if you perform error analysis and conclude that data mismatch is a huge source of error, how do you go about addressing that? It turns out that unfortunately there are super systematic ways to address data mismatch, but there are a few things you can try that could help. Let's take a look at them in the next video.\n",
    "\n",
    "<img align='center' src='images/data_mismatch.PNG' width='700'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing data mismatch\n",
    "\n",
    "If your training set comes from a different distribution, than your dev and test set, and if error analysis shows you that you have a data mismatch problem, what can you do?\n",
    "\n",
    "1. Carry out manual error analysis and try to understand the differences between the training set and the dev/test sets. To avoid overfitting the test set, technically for error analysis, you should manually only look at a dev set and not at the test set \n",
    "2. try to collect more data similar to your dev and test sets. \n",
    "\n",
    "So, to summarize, if you think you have a data mismatch problem, I recommend you do error analysis, or look at the training set, or look at the dev set to try this figure out, to try to gain insight into how these two distributions of data might differ. And then see if you can find some ways to get more training data that looks a bit more like your dev set. One of the ways we talked about is artificial data synthesis. And artificial data synthesis does work. In speech recognition, I've seen artificial data synthesis significantly boost the performance of what were already very good speech recognition system. So, it can work very well. But, if you're using artificial data synthesis, just be cautious and bear in mind whether or not you might be accidentally simulating data only from a tiny subset of the space of all possible examples. So, that's it for how to deal with data mismatch.\n",
    "\n",
    "# Transfer learning\n",
    "\n",
    "But if you have a lot of data, then maybe you can retrain all the parameters in the network. And if you retrain all the parameters in the neural network, then this initial phase of training on image recognition is sometimes called pre-training, because you're using image recognitions data to pre-initialize or really pre-train the weights of the neural network. And then if you are updating all the weights afterwards, then training on the radiology data sometimes that's called fine tuning.\n",
    "\n",
    "- Pre-training\n",
    "- FIne tuning\n",
    "\n",
    "And the reason this can be helpful is that a lot of the low level features such as detecting edges, detecting curves, detecting positive objects. Learning from that, from a very large image recognition data set, might help your learning algorithm do better in radiology diagnosis. It's just learned a lot about the structure and the nature of how images look like and some of that knowledge will be useful. So having learned to recognize images, it might have learned enough about you know, just what parts of different images look like, that that knowledge about lines, dots, curves, and so on, maybe small parts of objects, that knowledge could help your radiology diagnosis network learn a bit faster or learn with less data\n",
    "\n",
    "you're transferring from a problem with a lot of data to a problem with relatively little data. \n",
    "\n",
    "**When transfer learning makes sense**\n",
    "\n",
    "* Task A and B have the same input X\n",
    "* You have a lot more data for Task A than Task B\n",
    "* Low level features from A could be helpful for learning B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task learning\n",
    "\n",
    "\n",
    "So whereas in transfer learning, you have a sequential process where you learn from task A and then transfer that to task B. In multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. And then each of these task helps hopefully all of the other task. Let's look at an example.\n",
    "\n",
    " So to summarize, multi-task learning enables you to train one neural network to do many tasks and this can give you better performance than if you were to do the tasks in isolation. Now one note of caution, in practice I see that transfer learning is used much more often than multi-task learning. So I do see a lot of tasks where if you want to solve a machine learning problem but you have a relatively small data set, then transfer learning can really help. Where if you find a related problem but you have a much bigger data set, you can train in your neural network from there and then transfer it to the problem where we have very low data. So transfer learning is used a lot today. There are some applications of transfer multi-task learning as well, but multi-task learning I think is used much less often than transfer learning. And maybe the one exception is computer vision object detection,\n",
    " \n",
    " <img align='center' src='images/multi.PNG' width='900'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is end-to-end deep learning?\n",
    "\n",
    "Briefly, there have been some data processing systems, or learning systems that require multiple stages of processing. And what end-to-end deep learning does, is it can take all those multiple stages, and replace it usually with just a single neural network.\n",
    "\n",
    "Example: Face recognition\n",
    "\n",
    "first crop the face,  \n",
    "Then train ML to recognize the person.\n",
    "\n",
    "It is not a good approach to train ML to images where people is approaching to the camera\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Let the data speak\n",
    "* Less hand-designing of components needed\n",
    "\n",
    "**Cons**\n",
    "* May need large amount of data\n",
    "* Excludes potentially useful hand-designed components\n",
    "\n",
    "\n",
    "##  Whether to use end-to-end deep learning\n",
    "\n",
    "\n",
    "*  Use DL to learn individual components\n",
    "* when applying supervised learning you should carefully choose what types of X to Y mappings you want to learn depending on what task you can get data fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
